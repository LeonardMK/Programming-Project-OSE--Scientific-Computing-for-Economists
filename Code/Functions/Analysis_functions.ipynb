{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for plotting benchmarking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygmo as pg\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showcase on how to use the Pygmo library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters first\n",
    "int_gen = 100\n",
    "int_pop_size = 100\n",
    "int_seed = 209311\n",
    "int_verbosity = 1\n",
    "list_algo_name = [\"cmaes\", \"gaco\", \"de\"]\n",
    "\n",
    "# Example problems can be called here. Can define UDPs too.\n",
    "# A problem needs to be constructed with pg.problem\n",
    "prob_rosenbrock = pg.problem(pg.rosenbrock(5))\n",
    "\n",
    "# Most used methods are fitness and get_(f/g/h)evals. Can set a seed.\n",
    "\n",
    "# User defined algorithms are provided by the algorithm type\n",
    "algo_cmaes = pg.algorithm(pg.cmaes(int_gen, seed = int_seed))\n",
    "algo_gaco = pg.algorithm(pg.gaco(int_gen, seed = int_seed))\n",
    "algo_de = pg.algorithm(pg.de(int_gen, seed = int_seed))\n",
    "\n",
    "# To benchmark, we need to set a value > 0 for verbosity to generate a log file\n",
    "algo_cmaes.set_verbosity(int_verbosity)\n",
    "algo_gaco.set_verbosity(int_verbosity)\n",
    "algo_de.set_verbosity(int_verbosity)\n",
    "\n",
    "# The population is a pool of candidate solution to the problem\n",
    "pop_rosenbrock = pg.population(prob_rosenbrock, size = 100, seed = int_seed)\n",
    "\n",
    "# Can get the number of function evaluations by using\n",
    "pop_rosenbrock.problem.get_fevals\n",
    "\n",
    "# Islands are used to allow multithread computation\n",
    "# size gives the number of function evaluations\n",
    "isl_rosenbrock = pg.island(algo = algo_cmaes, prob = prob_rosenbrock, size = 20)\n",
    "\n",
    "# Archipelage is used for parallelization and allow for migration of solutions. \n",
    "# Might be useful as a special case and how this can help.\n",
    "\n",
    "# Solve rosenbrock using three algorithms\n",
    "pop_rosenbrock_cmaes = algo_cmaes.evolve(pop_rosenbrock)\n",
    "pop_rosenbrock_gaco = algo_gaco.evolve(pop_rosenbrock)\n",
    "pop_rosenbrock_de = algo_de.evolve(pop_rosenbrock)\n",
    "\n",
    "# Get the log files as a pandas dataframe\n",
    "log_cmaes_rosenbrock = algo_cmaes.extract(pg.cmaes)\n",
    "log_gaco_rosenbrock = algo_gaco.extract(pg.gaco)\n",
    "log_de_rosenbrock = algo_de.extract(pg.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- How to take into account, that parameter tuning was used?\n",
    "    - Could start from default settings.\n",
    "- Since starting points are random should repeat this many times. Can implement the same seed for all algos.\n",
    "- How to find out, when the stopping criterion was reached?\n",
    "    - Lookup log file and find the number of function iterations.\n",
    "- Would also like to show how the population size affects performance.\n",
    "- Since all optimums are known other stopping conditions can be turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarker(problem, algorithms, gen = 1000, pop_size = 100, seed = 2093111, verbosity = 1\n",
    "               **kwargs_fun, **kwargs_algo):\n",
    "    \n",
    "    # Number of log entries for every algorithm\n",
    "    int_no_entries = gen / verbosity\n",
    "    \n",
    "    # Get the problem\n",
    "    class_problem = pg.problem(getattr(pg, problem))\n",
    "    \n",
    "    # Generate a population that with size equal pop_size\n",
    "    population = pg.population(class_problem, size = pop_size, seed = seed)\n",
    "    \n",
    "    # Define a pandas dataframe in which to insert the logs\n",
    "    # index = algo, columns is entries\n",
    "    list_columns = [\"gen\", \"fevals\", \"best\", \"dx\", \"df\", \"sigma\"]\n",
    "    array_index = np.repeat(algorithms, int_no_entries)\n",
    "    df_logs = pd.DataFrame(\n",
    "        np.empty(shape = (int_no_entries, 6)),\n",
    "        index = array_index,\n",
    "        columns = \n",
    "    )\n",
    "    \n",
    "    # Define a second dataframe in which to insert fevals, gevals and hevals\n",
    "    \n",
    "    # Now run evolve for every algo in algorithms\n",
    "    for algo in algortihms:\n",
    "        \n",
    "        # Use getattr\n",
    "        \n",
    "        # Need to set verbosity\n",
    "        \n",
    "        # Return the log file as a series\n",
    "        \n",
    "        # Calculate difference between f(x_sol) and f(x_champ)\n",
    "        \n",
    "        # Calculate distance between x_sol and x_champ\n",
    "        \n",
    "        # Normalize by dividing by starting value\n",
    "        \n",
    "        # log_10 differences\n",
    "        \n",
    "        # Calculation of violated constraints\n",
    "        # Different metrics: sum, squared sum, mean, geometric mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some test models first on functions from the Pygmo package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to run the Test_functions nb to get the functions\n",
    "%run Test_functions.ipynb\n",
    "\n",
    "# Selected methods\n",
    "scipy_lo_methods = [\"L-BFGS-B\", \"Nelder-Mead\"]\n",
    "scipy_go_methods = [\"basinhopping\", \"brute\", \"differential_evolution\", \"shgo\"]\n",
    "pyOpt_methods = [\"NSGA2\", \"ALPSO\"]\n",
    "all_methods = scipy_lo_methods + scipy_go_methods + pyOpt_methods\n",
    "\n",
    "# Need a function that returns a tuple containing lower and upper bounds\n",
    "def create_bounds(lower, upper):\n",
    "    \n",
    "    if len(lower) != len(upper):\n",
    "        raise TypeError(\"The length of lower ({0}) and upper ({1}) differ.\".format(len(lower), len(upper)))\n",
    "        \n",
    "    if lower > upper:\n",
    "        raise ValueError(\"The value of at least one lower bound, exceeds that of an upper bound.\")\n",
    "    \n",
    "    dim_bounds = len(lower)    \n",
    "    bounds = list()\n",
    "    for i in range(dim_bounds):\n",
    "        if np.abs(lower[i]) == np.inf:\n",
    "            lb = None\n",
    "        else:\n",
    "            lb = lower[i]\n",
    "        if np.abs(upper[i]) == np.inf:\n",
    "            ub = None\n",
    "        else:\n",
    "            ub = upper[i]\n",
    "        \n",
    "        bounds.append((lb, ub))\n",
    "    \n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Need to get some solved optimization algorithms first. Test with the Ackley function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ack = Ackley()\n",
    "x0 = np.random.uniform(low = -5, high = 5, size = 2)\n",
    "bounds = create_bounds(ack.lb, ack.ub)\n",
    "bh_bounds = bh_Bounds(ack.lb, ack.ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bh = opt.basinhopping(ack.fun, x0, minimizer_kwargs = {\"method\": \"L-BFGS-B\", \"bounds\": bounds})\n",
    "res_shgo = opt.shgo(ack.fun, bounds)\n",
    "res_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory Plot\n",
    "\n",
    "A trajectory plot shows how the algorithm moved from iteration to iteration. Want to have one plot for multiple algorithms. EA need a single plot with multiple points. Since all algos start from multiple points it could be wise to have an animated plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Plot\n",
    "\n",
    "Plots the best function value against the number of function evaluations. An average runtime plot could be useful. For example how it changes with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance profiles\n",
    "For a given set of problems $ \\mathcal{P} $, solvers $ \\mathcal{S} $ and a convergence test $ \\mathcal{T} $. They are a performance measre $ t_{p, s} > 0 $, where $ p,\\, s $ are indices for a given problem and solver $ (p, s) \\in \\mathcal{P} \\times \\mathcal{S} $. Now the value $ r_{p, s} $ is defined as:\n",
    "\n",
    "$$ r_{p, s} = \\begin{cases} \n",
    "\\frac{t_{p, s}}{\\min\\{t_{p, s}: \\, s \\in \\mathcal{S} \\}} \\quad \\text{if convergence is passed} \\\\\n",
    "\\infty \\quad \\text{if convergence fails}\n",
    "\\end{cases} $$\n",
    "\n",
    "The best performing optimizer will have $ r_{p, s} = 1 $. For a specific problem $ p $ and cutoff $ \\tau > 1 $, the performance profile for solver $ s $ is defined as follows:\n",
    "\n",
    "$$ \\rho_s(\\tau) = \\frac{1}{|\\mathcal{P}|} \\text{size} \\{p \\in \\mathcal{P}: \\, r_{p, s} \\leq \\tau \\} $$\n",
    "\n",
    "where $ |\\mathcal{P} | $ is the cardinality of the set $ \\mathcal{P} $. $ \\rho_s(1) $ represents the fraction that solver $ s $ is the best performing solver. Want to identify solvers with high values. Remember that performance profiles depend of the solvers considered. For comparing only two optimizers a new profile should be drawn.\n",
    "\n",
    "Important is what measure we consider for performance. Possible are function value or distance to optimal solution. For measuring performance by function value\n",
    "\n",
    "$$ m_{p, s} = \\frac{\\hat{f}_{p, s}(k) - f^*}{f_w - f^*} $$\n",
    "\n",
    "where $ k $ is the value of function evaluations and $ f_w $ is the worst value after $ k $ evaluations.\n",
    "\n",
    "Performance profiles can allow to assess both speed and success rate. However, caution is advised as it is not ceretain that the best solution found is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Profiles\n",
    "Visualize an entire benchmarking test set. Only applicable for fixed-cost problems. For every $ s \\in \\mathcal{S} $ and $ p \\in \\mathcal{P} $, we calculate an accuracy measure is calculated, where $ M $ is a maximum improvement value. \n",
    "\n",
    "$$ \\gamma_{p, s} = \\begin{cases}\n",
    "-f_{acc}^{p, s}, \\quad \\text{if } -f_{acc}^{p, s} \\leq M \\\\\n",
    "M, \\quad -f_{acc}^{p, s} > M \\text{ or } f_{acc}^{p, s} \\text{ is undefined}\n",
    "\\end{cases} $$\n",
    "\n",
    "where $ f_{acc}^{p, s} = \\log_{10}(f(\\bar{x}_{p, s} - f(x_p^*)) - \\log_{10}(f(x_p^0) - f(x_p^*)) $, $ x_{p, s} $ is the candidate solution, obtained by solver $ s $ for problem $ p $, $ x_p^* $ is the optimal point for problem $ p $, and $ x_p^0 $ is the initial point for problem $ p $. To measure the performance we calculate \n",
    "\n",
    "$$ R_s (\\tau) = \\frac{1}{|\\mathcal{P}|} \\text{size} \\{ \\gamma_{p, s} | \\gamma_{p, s} \\geq \\tau, \\, p \\in \\mathcal{P} \\} $$\n",
    "\n",
    "it shows the proportion of problems for which the solver $ s $ achieves an accuracy of at least $ \\tau $. Only suitable for fixed cost data-sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Profiles\n",
    "Proposed for derivative-free optimization algorithms. They show, which percentage of problems for a given value $ \\tau $ can be solved within the budget of $ k $ function evaluations. Since it is assumed, that the number of functions evluations grows with the dimension of the problem $ n_p $, it is defined as\n",
    "\n",
    "$$ d_s (k) = \\frac{1}{\\mathcal{P}} \\text{size} \\{p \\in \\mathcal{P}: \\, \\frac{t_{p, s}}{n_p + 1} \\leq k} $$\n",
    "\n",
    "here, $ t_{p, s} $ is the number of function evaluations required to satisfy the convergence test, $ d_s (k) $ is the fraction of problems $ p $ solved by $ s $ within $ k $ evaluations. Data profiles are independent of other solvers. To compare gradient-free and gradient-based methods I will also implement a data profiles that accounts for number of gradient and hessian evaluations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
